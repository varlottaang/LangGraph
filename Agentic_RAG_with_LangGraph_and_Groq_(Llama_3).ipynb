{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30673,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Agentic RAG with LangGraph and Groq (Llama 3)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varlottaang/LangGraph/blob/main/Agentic_RAG_with_LangGraph_and_Groq_(Llama_3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"\n",
        "  font-size: 3em;\n",
        "  color: #333;\n",
        "  text-align: center;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 20px;\n",
        "  border-radius: 10px;\n",
        "  margin: 40px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "\">Agentic RAG with LangGrapgh ðŸŽ‰</h1>"
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "MXKM1C66hfCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color: #f9f9f9; padding: 20px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\">\n",
        "  <p style=\"font-size: 16px; line-height: 1.6; color: #555;\">\n",
        "    In this notebook, I built a RAG (Retrieval Augmented Generation) system to answer medical queries. The agent's primary duty is to fetch chunks from a vector store to answer a user's query. However, the retrieved chunks are not just given as-is to the language model (LLM). They are first scored, and only the chunks that are relevant to the query are passed to the LLM.\n",
        "  </p>\n",
        "  <p style=\"font-size: 16px; line-height: 1.6; color: #555;\">\n",
        "    In cases where all the retrieved chunks are not relevant to the query, the agent performs a web search. The agent also performs a web search for queries that do not have any relevant documents in the vector store.\n",
        "  </p>\n",
        "  <p style=\"font-size: 16px; line-height: 1.6; color: #555;\">\n",
        "    Before the LLM's response is presented to the user, the system performs a hallucination and answer relevance check. Lastly, if the query is not related to health, the system falls back to a different chain to respond based on its knowledge.\n",
        "  </p>\n",
        "  <h3 style=\"color: #333; margin-top: 30px;\">Stack Used:</h3>\n",
        "  <ul style=\"font-size: 16px; line-height: 1.6; color: #555; list-style-type: none; padding-left: 0;\">\n",
        "    <li>- Chromadb</li>\n",
        "    <li>- Tavily AI</li>\n",
        "    <li>- HuggingFaceHubEmbeddings (sentence-transformers/all-mpnet-base-v2)</li>\n",
        "    <li>- Llama 3 70B from Groq</li>\n",
        "    <li>- Langchain and LangGraph</li>\n",
        "    <li>- Gradio</li>\n",
        "  </ul>\n",
        "</div>"
      ],
      "metadata": {
        "id": "aRt5dLvbhfCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"\n",
        "  font-size: 2em;\n",
        "  color: #333;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 15px;\n",
        "  border-radius: 10px;\n",
        "  margin: 20px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "  width: 420px;\n",
        "\">1. Importing relevant libraries</h1>"
      ],
      "metadata": {
        "id": "NLZmkdbZhfCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain\\\n",
        "    langchain-community\\\n",
        "    langchain-groq\\\n",
        "    langchain-core\\\n",
        "    gpt4all\\\n",
        "    langgraph\\\n",
        "    chromadb\\\n",
        "    sentence-transformers\\\n",
        "    tavily-python\\\n",
        "    gradio"
      ],
      "metadata": {
        "trusted": true,
        "id": "VaJwstWyhfCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders.web_base import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import nest_asyncio\n",
        "from langchain_community.vectorstores.chroma import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_groq.chat_models import ChatGroq\n",
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Literal\n",
        "from langchain.chains.combine_documents import stuff\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from IPython.core.display import Markdown\n",
        "import json\n",
        "import re\n",
        "from langchain_core.runnables import (\n",
        "    RunnableParallel,\n",
        "    RunnableBranch,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from operator import itemgetter\n",
        "import asyncio"
      ],
      "metadata": {
        "trusted": true,
        "id": "wbOi3slQhfCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"\n",
        "  font-size: 2em;\n",
        "  color: #333;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 15px;\n",
        "  border-radius: 10px;\n",
        "  margin: 20px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "  width: 400px;\n",
        "\">2. Set Environment Variables</h1>"
      ],
      "metadata": {
        "id": "Z9DO-IyjhfCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "import os\n",
        "user_secrets = UserSecretsClient()\n",
        "GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
        "LANGCHAIN_API_KEY = user_secrets.get_secret(\"LANGCHAIN_API_KEY\")\n",
        "LANGCHAIN_PROJECT = user_secrets.get_secret(\"LANGCHAIN_PROJECT\")\n",
        "TAVILY_API_KEY = user_secrets.get_secret(\"TAVILY_API_KEY\")\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]=LANGCHAIN_API_KEY\n",
        "os.environ[\"LANGCHAIN_PROJECT\"]=\"Agentic RAG\"\n",
        "os.environ[\"TAVILY_API_KEY\"]=TAVILY_API_KEY"
      ],
      "metadata": {
        "trusted": true,
        "id": "Djpx99gkhfCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility Function"
      ],
      "metadata": {
        "id": "Vs8jwDkohfCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_search_research(results: str):\n",
        "    pattern = r\"\\[content: (.*?), title: (.*?), url: (.*?)\\]\"\n",
        "    result = re.findall(pattern, results)\n",
        "\n",
        "    data_list = []\n",
        "    for snippet, title, link in result:\n",
        "        data_list.append({\"content\": snippet, \"title\": title, \"url\": link})\n",
        "    return data_list"
      ],
      "metadata": {
        "trusted": true,
        "id": "kEp5bX5IhfCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"\n",
        "  font-size: 2em;\n",
        "  color: #333;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 15px;\n",
        "  border-radius: 10px;\n",
        "  margin: 20px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "  width: 600px;\n",
        "\">3. Preparing and Storing Data in Vector Store.</h1>"
      ],
      "metadata": {
        "id": "wGkmc6XxhfCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://www.webmd.com/a-to-z-guides/malaria\",\n",
        "    \"https://www.webmd.com/diabetes/type-1-diabetes\",\n",
        "    \"https://www.webmd.com/diabetes/type-2-diabetes\",\n",
        "    \"https://www.webmd.com/migraines-headaches/migraines-headaches-migraines\",\n",
        "]\n",
        "\n",
        "loader = WebBaseLoader(urls, bs_get_text_kwargs={\"strip\": True})\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Vdw8M8fkhfCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "embedding_function = HuggingFaceEmbeddings(show_progress=True, multi_process=True)\n",
        "\n",
        "vector_store = Chroma.from_documents(documents=chunks, embedding=embedding_function)\n",
        "\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "trusted": true,
        "id": "V_KKz-RyhfCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retriever.get_relevant_documents(\"Symptoms of migraine\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "sAL01UNjhfCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  font-size: 1em;\n",
        "  color: #333;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 10px;\n",
        "  border-radius: 10px;\n",
        "  margin: 20px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "  width: 500px;\n",
        "\">\n",
        "    <h1>4. Question Router Chain.</h1>\n",
        "    <p style=\"font-size: 1.2em;\"\n",
        "        >This will direct the user's query to one of the following: a vector store, a web search engine, or neither (None).</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "3riWHp82hfCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore(BaseModel):\n",
        "    (\n",
        "        \"A vectorstore contains information about symptoms, treatment\"\n",
        "        \", risk factors and other information about malaria, type 1 and\"\n",
        "        \"type 2 diabetes and migraines\"\n",
        "    )\n",
        "\n",
        "    query: str\n",
        "\n",
        "\n",
        "class SearchEngine(BaseModel):\n",
        "    \"\"\"A search engine for searching other medical information on the web\"\"\"\n",
        "\n",
        "    query: str\n",
        "\n",
        "class SearchEngine(BaseModel):\n",
        "    \"\"\"A search engine for searching other medical information on the web\"\"\"\n",
        "\n",
        "    query: str\n",
        "\n",
        "router_prompt_template = (\n",
        "    \"You are an expert in routing user queries to either a VectorStore, SearchEngine\\n\"\n",
        "    \"Use SearchEngine for all other medical queries that are not related to malaria, diabetes, or migraines.\\n\"\n",
        "    \"The VectorStore contains information on malaria, diabetes, and migraines.\\n\"\n",
        "    'Note that if a query is not medically-related, you must output \"not medically-related\", don\\'t try to use any tool.\\n\\n'\n",
        "    \"query: {query}\"\n",
        ")\n",
        "\n",
        "llm = ChatGroq(model=\"Llama3-70b-8192\", temperature=0)\n",
        "prompt = ChatPromptTemplate.from_template(router_prompt_template)\n",
        "question_router = prompt | llm.bind_tools(tools=[VectorStore, SearchEngine])"
      ],
      "metadata": {
        "trusted": true,
        "id": "cLgKISeAhfCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = question_router.invoke(\"What are the symptoms of chest pain?\")\n",
        "\"tool_calls\" in response.additional_kwargs"
      ],
      "metadata": {
        "trusted": true,
        "id": "p7fBR9jFhfCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZARwMjrRhfCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  font-size: 1em;\n",
        "  color: #333;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 10px;\n",
        "  border-radius: 10px;\n",
        "  margin: 20px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "  width: 600px;\n",
        "\">\n",
        "    <h1>5. Retrieved Documents Relevance Grader</h1>\n",
        "    <p style=\"font-size: 1.2em;\"\n",
        "        >This is to make sure we filter the documents down so that only the relevant chunks are used as context and not the entire retrieved chunks.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "zBBbOgWihfCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import validator\n",
        "\n",
        "\n",
        "class Grader(BaseModel):\n",
        "    \"Use this format to give a binary score for relevance check on retrived documents.\"\n",
        "\n",
        "    grade: Literal[\"relevant\", \"irrelevant\"] = Field(\n",
        "        ...,\n",
        "        description=\"The relevance score for the document.\\n\"\n",
        "        \"Set this to 'relevant' if the given context is relevant to the user's query, or 'irrlevant' if the document is not relevant.\",\n",
        "    )\n",
        "\n",
        "    @validator(\"grade\", pre=True)\n",
        "    def validate_grade(cls, value):\n",
        "        if value == \"not relevant\":\n",
        "            return \"irrelevant\"\n",
        "        return value\n",
        "\n",
        "\n",
        "grader_system_prompt_template = \"\"\"\"You are a grader tasked with assessing the relevance of a given context to a query.\n",
        "    If the context is relevant to the query, score it as \"relevant\". Otherwise, give \"irrelevant\".\n",
        "    Do not answer the actual answer, just provide the grade in JSON format with \"grade\" as the key, without any additional explanation.\"\n",
        "    \"\"\"\n",
        "\n",
        "grader_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", grader_system_prompt_template),\n",
        "        (\"human\", \"context: {context}\\n\\nquery: {query}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "grader_chain = grader_prompt | llm.with_structured_output(Grader, method=\"json_mode\")\n",
        "\n",
        "query = \"symptoms of migraine\"\n",
        "context = retriever.get_relevant_documents(query)\n",
        "\n",
        "response = grader_chain.invoke({\"query\": query, \"context\": context})"
      ],
      "metadata": {
        "trusted": true,
        "id": "kWP3cpKbhfCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "trusted": true,
        "id": "3_eKopsXhfCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Treatment of Ulcer\"\n",
        "context = retriever.get_relevant_documents(query)\n",
        "\n",
        "response = grader_chain.invoke({\"query\": query, \"context\": context})\n",
        "print(response)"
      ],
      "metadata": {
        "trusted": true,
        "id": "K6wFSexfhfCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  font-size: 1em;\n",
        "  color: #333;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 10px;\n",
        "  border-radius: 10px;\n",
        "  margin: 20px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "  width: 500px;\n",
        "\">\n",
        "    <h1>5. RAG Chain</h1>\n",
        "    <p style=\"font-size: 1.2em;\"\n",
        "        >Responding to the user's query based on the filtered chunks</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "vHkRJ8awhfCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_template_str = (\n",
        "    \"You are a helpful assistant. Answer the query below based only on the provided context.\\n\\n\"\n",
        "    \"context: {context}\\n\\n\"\n",
        "    \"query: {query}\"\n",
        ")\n",
        "\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(rag_template_str)\n",
        "rag_chain = rag_prompt | llm | StrOutputParser()\n",
        "\n",
        "query = \"What are the symptoms of malaria?\"\n",
        "context = retriever.get_relevant_documents(query)\n",
        "\n",
        "response = rag_chain.invoke({\"query\": query, \"context\": context})\n",
        "\n",
        "Markdown(response)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4fpvx1pyhfCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  font-size: 1em;\n",
        "  color: #333;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 10px;\n",
        "  border-radius: 10px;\n",
        "  margin: 20px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "  width: 500px;\n",
        "\">\n",
        "    <h1>6. Fallback Chain</h1>\n",
        "    <p style=\"font-size: 1.2em;\"\n",
        "        >This chain is responsible for handling situations where the agent does not call a tool bacause calling a tool is not relevant.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "TxaV2QLnhfCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fallback_prompt = ChatPromptTemplate.from_template(\n",
        "    (\n",
        "        \"You are a friendly medical assistant created by RedxAI.\\n\"\n",
        "        \"Do not respond to queries that are not related to health.\\n\"\n",
        "        \"If a query is not related to health, acknowledge your limitations.\\n\"\n",
        "        \"Provide concise responses to only medically-related queries.\\n\\n\"\n",
        "        \"Current conversations:\\n\\n{chat_history}\\n\\n\"\n",
        "        \"human: {query}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "fallback_chain = (\n",
        "    {\n",
        "        \"chat_history\": lambda x: \"\\n\".join(\n",
        "            [\n",
        "                (\n",
        "                    f\"human: {msg.content}\"\n",
        "                    if isinstance(msg, HumanMessage)\n",
        "                    else f\"AI: {msg.content}\"\n",
        "                )\n",
        "                for msg in x[\"chat_history\"]\n",
        "            ]\n",
        "        ),\n",
        "        \"query\": itemgetter(\"query\") ,\n",
        "    }\n",
        "    | fallback_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "fallback_chain.invoke(\n",
        "    {\n",
        "        \"query\": \"Hello\",\n",
        "        \"chat_history\": [],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "13jVLpWNhfCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  font-size: 1em;\n",
        "  color: #333;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 10px;\n",
        "  border-radius: 10px;\n",
        "  margin: 20px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "  width: 720px;\n",
        "\">\n",
        "    <h1>7. Hallucination and Answer Relevance Assessment</h1>\n",
        "    <p style=\"font-size: 1.2em;\"\n",
        "        >In the final step, we use the <strong>LLM</strong> to evaluate whether its response is a hallucination or not, based on the\n",
        "        response itself and the provided context. If the response is not derived from the given context, it is marked as a hallucination.\n",
        "        Once the hallucination check is passed, the user's query and the LLM's response are then assessed to determine if the response\n",
        "        relevant to the query.<br><br>\n",
        "        If the response is identified as a hallucination, we go back to the <strong>RAG chain</strong>. If the response is not\n",
        "        hallucination, but the answer relevance check fails, we perform a web search. Otherwise, we return the LLM's response to the user\n",
        "    </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "RiZoLENQhfCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HallucinationGrader(BaseModel):\n",
        "    \"Binary score for hallucination check in llm's response\"\n",
        "\n",
        "    grade: Literal[\"yes\", \"no\"] = Field(\n",
        "        ..., description=\"'yes' if the llm's reponse is hallucinated otherwise 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "hallucination_grader_system_prompt_template = (\n",
        "    \"You are a grader assessing whether a response from an llm is based on a given context.\\n\"\n",
        "    \"If the llm's response is not based on the given context give a score of 'yes' meaning it's a hallucination\"\n",
        "    \"otherwise give 'no'\\n\"\n",
        "    \"Just give the grade in json with 'grade' as a key and a binary value of 'yes' or 'no' without additional explanation\"\n",
        ")\n",
        "\n",
        "hallucination_grader_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", hallucination_grader_system_prompt_template),\n",
        "        (\"human\", \"context: {context}\\n\\nllm's response: {response}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "hallucination_grader_chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"response\": itemgetter(\"response\"),\n",
        "            \"context\": lambda x: \"\\n\\n\".join([c.page_content for c in x[\"context\"]]),\n",
        "        }\n",
        "    )\n",
        "    | hallucination_grader_prompt\n",
        "    | llm.with_structured_output(HallucinationGrader, method=\"json_mode\")\n",
        ")\n",
        "\n",
        "query = \"Symptoms of malaria\"\n",
        "context = retriever.get_relevant_documents(query)\n",
        "response = \"\"\"Based on the context provided, the symptoms of malaria include: Impaired consciousness, Convulsions, Difficulty breathing,\n",
        "Serious tiredness and fatigue, Dark or bloody urine, Yellow eyes and skin (jaundice), Abnormal bleeding, High fever, Chills,\n",
        "Sweating, Nausea or vomiting, Headache, Diarrhea\"\"\"\n",
        "\n",
        "response = hallucination_grader_chain.invoke({\"response\": response, \"context\": context})"
      ],
      "metadata": {
        "trusted": true,
        "id": "IcnRfWpIhfCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "trusted": true,
        "id": "2TDRiAxhhfCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerGrader(BaseModel):\n",
        "    \"Binary score for an answer check based on a query.\"\n",
        "\n",
        "    grade: Literal[\"yes\", \"no\"] = Field(\n",
        "        ...,\n",
        "        description=\"'yes' if the provided answer is an actual answer to the query otherwise 'no'\",\n",
        "    )\n",
        "\n",
        "\n",
        "answer_grader_system_prompt_template = (\n",
        "    \"You are a grader assessing whether a provided answer is in fact an answer to the given query.\\n\"\n",
        "    \"If the provided answer does not answer the query give a score of 'no' otherwise give 'yes'\\n\"\n",
        "    \"Just give the grade in json with 'grade' as a key and a binary value of 'yes' or 'no' without additional explanation\"\n",
        ")\n",
        "\n",
        "answer_grader_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", answer_grader_system_prompt_template),\n",
        "        (\"human\", \"query: {query}\\n\\nanswer: {response}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "answer_grader_chain = answer_grader_prompt | llm.with_structured_output(\n",
        "    AnswerGrader, method=\"json_mode\"\n",
        ")\n",
        "\n",
        "query = \"Symptoms of malaria\"\n",
        "# context = retriever.get_relevant_documents(query)\n",
        "response = \"\"\"Based on the context provided, the symptoms of malaria include: Impaired consciousness, Convulsions, Difficulty breathing,\n",
        "Serious tiredness and fatigue, Dark or bloody urine, Yellow eyes and skin (jaundice), Abnormal bleeding, High fever, Chills,\n",
        "Sweating, Nausea or vomiting, Headache, Diarrhea\"\"\"\n",
        "\n",
        "response = answer_grader_chain.invoke({\"response\": response, \"query\": query})"
      ],
      "metadata": {
        "trusted": true,
        "id": "M7Lijgu1hfCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "trusted": true,
        "id": "eBee5Jk_hfCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  font-size: 1em;\n",
        "  color: #333;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 10px;\n",
        "  border-radius: 10px;\n",
        "  margin: 20px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "  width: 500px;\n",
        "\">\n",
        "    <h1>8. Defining the Agent's Workflow</h1>\n",
        "</div>"
      ],
      "metadata": {
        "id": "UZrT3pblhfCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "from langgraph.prebuilt import ToolInvocation, ToolExecutor\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_core.messages.base import BaseMessage\n",
        "import operator\n",
        "\n",
        "# ddg_search = DuckDuckGoSearchResults()\n",
        "tavily_search = TavilySearchResults()\n",
        "tool_executor = ToolExecutor(\n",
        "    tools=[\n",
        "        Tool(\n",
        "            name=\"VectorStore\",\n",
        "            func=retriever.invoke,\n",
        "            description=\"Useful to search the vector database\",\n",
        "        ),\n",
        "        Tool(\n",
        "            name=\"SearchEngine\", func=tavily_search, description=\"Useful to search the web\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "class AgentSate(TypedDict):\n",
        "    \"\"\"The dictionary keeps track of the data required by the various nodes in the graph\"\"\"\n",
        "\n",
        "    query: str\n",
        "    chat_history:list[BaseMessage]\n",
        "    generation: str\n",
        "    documents: list[Document]\n",
        "\n",
        "\n",
        "def retrieve_node(state: dict) -> dict[str, list[Document] | str]:\n",
        "    \"\"\"\n",
        "    Retrieve relevent documents from the vectorstore\n",
        "\n",
        "    query: str\n",
        "\n",
        "    return list[Document]\n",
        "    \"\"\"\n",
        "    query = state[\"query\"]\n",
        "    documents = retriever.invoke(input=query)\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "\n",
        "def fallback_node(state: dict):\n",
        "    \"\"\"\n",
        "    Fallback to this node when there is no tool call\n",
        "    \"\"\"\n",
        "    query = state[\"query\"]\n",
        "    chat_history = state[\"chat_history\"]\n",
        "    generation = fallback_chain.invoke({\"query\": query, \"chat_history\": chat_history})\n",
        "    return {\"generation\": generation}\n",
        "\n",
        "\n",
        "def filter_documents_node(state: dict):\n",
        "    filtered_docs = list()\n",
        "\n",
        "    query = state[\"query\"]\n",
        "    documents = state[\"documents\"]\n",
        "    for i, doc in enumerate(documents, start=1):\n",
        "        grade = grader_chain.invoke({\"query\": query, \"context\": doc})\n",
        "        if grade.grade == \"relevant\":\n",
        "            print(f\"---CHUCK {i}: RELEVANT---\")\n",
        "            filtered_docs.append(doc)\n",
        "        else:\n",
        "            print(f\"---CHUCK {i}: NOT RELEVANT---\")\n",
        "    return {\"documents\": filtered_docs}\n",
        "\n",
        "\n",
        "def rag_node(state: dict):\n",
        "    query = state[\"query\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    generation = rag_chain.invoke({\"query\": query, \"context\": documents})\n",
        "    return {\"generation\": generation}\n",
        "\n",
        "\n",
        "def web_search_node(state: dict):\n",
        "    query = state[\"query\"]\n",
        "    results = tavily_search.invoke(query)\n",
        "    # results = parse_search_research(results)\n",
        "    documents = [\n",
        "        Document(page_content=doc[\"content\"], metadata={\"source\": doc[\"url\"]})\n",
        "        for doc in results\n",
        "    ]\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "\n",
        "def question_router_node(state: dict):\n",
        "    query = state[\"query\"]\n",
        "    try:\n",
        "        response = question_router.invoke({\"query\": query})\n",
        "    except Exception:\n",
        "        return \"llm_fallback\"\n",
        "\n",
        "    if \"tool_calls\" not in response.additional_kwargs:\n",
        "        print(\"---No tool called---\")\n",
        "        return \"llm_fallback\"\n",
        "\n",
        "    if len(response.additional_kwargs[\"tool_calls\"]) == 0:\n",
        "        raise \"Router could not decide route!\"\n",
        "\n",
        "    route = response.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]\n",
        "    if route == \"VectorStore\":\n",
        "        print(\"---Routing to VectorStore---\")\n",
        "        return \"VectorStore\"\n",
        "    elif route == \"SearchEngine\":\n",
        "        print(\"---Routing to SearchEngine---\")\n",
        "        return \"SearchEngine\"\n",
        "\n",
        "\n",
        "def should_generate(state: dict):\n",
        "    filtered_docs = state[\"documents\"]\n",
        "\n",
        "    if not filtered_docs:\n",
        "        print(\"---All retrived documents not relevant---\")\n",
        "        return \"SearchEngine\"\n",
        "    else:\n",
        "        print(\"---Some retrived documents are relevant---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "def hallucination_and_answer_relevance_check(state: dict):\n",
        "    llm_response = state[\"generation\"]\n",
        "    documents = state[\"documents\"]\n",
        "    query = state[\"query\"]\n",
        "\n",
        "    hallucination_grade = hallucination_grader_chain.invoke(\n",
        "        {\"response\": llm_response, \"context\": documents}\n",
        "    )\n",
        "    if hallucination_grade.grade == \"no\":\n",
        "        print(\"---Hallucination check passed---\")\n",
        "        answer_relevance_grade = answer_grader_chain.invoke(\n",
        "            {\"response\": llm_response, \"query\": query}\n",
        "        )\n",
        "        if answer_relevance_grade.grade == \"yes\":\n",
        "            print(\"---Answer is relevant to question---\\n\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---Answer is not relevant to question---\")\n",
        "            return \"not useful\"\n",
        "    print(\"---Hallucination check failed---\")\n",
        "    return \"generate\""
      ],
      "metadata": {
        "trusted": true,
        "id": "xZpkHKTXhfCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentSate)\n",
        "workflow.add_node(\"VectorStore\", retrieve_node)\n",
        "workflow.add_node(\"SearchEngine\", web_search_node)\n",
        "workflow.add_node(\"filter_docs\", filter_documents_node)\n",
        "workflow.add_node(\"fallback\", fallback_node)\n",
        "workflow.add_node(\"rag\", rag_node)\n",
        "\n",
        "workflow.set_conditional_entry_point(\n",
        "    question_router_node,\n",
        "    {\n",
        "        \"llm_fallback\": \"fallback\",\n",
        "        \"VectorStore\": \"VectorStore\",\n",
        "        \"SearchEngine\": \"SearchEngine\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"VectorStore\", \"filter_docs\")\n",
        "workflow.add_edge(\"SearchEngine\", \"filter_docs\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"filter_docs\", should_generate, {\"SearchEngine\": \"SearchEngine\", \"generate\": \"rag\"}\n",
        ")\n",
        "workflow.add_conditional_edges(\n",
        "    \"rag\",\n",
        "    hallucination_and_answer_relevance_check,\n",
        "    {\"useful\": END, \"not useful\": \"SearchEngine\", \"generate\": \"rag\"},\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"fallback\", END)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-mEMIlfRhfCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile(debug=False)\n",
        "plot = app.get_graph().draw_mermaid_png()\n",
        "\n",
        "with open(\"plot.png\", \"wb\") as fp:\n",
        "    fp.write(plot)"
      ],
      "metadata": {
        "trusted": true,
        "id": "eoqPq7L6hfCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Assuming you have the byte image stream in the 'byte_image' variable\n",
        "img = Image.open(BytesIO(plot))\n",
        "display(img)"
      ],
      "metadata": {
        "trusted": true,
        "id": "eEoMI59qhfCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  font-size: 1em;\n",
        "  color: #333;\n",
        "  background-color: #f2f2f2;\n",
        "  padding: 10px;\n",
        "  border-radius: 10px;\n",
        "  margin: 20px;\n",
        "  animation: fadeIn 1s ease-in-out;\n",
        "  width: 200px;\n",
        "\">\n",
        "    <h1>9. Testing</h1>\n",
        "</div>"
      ],
      "metadata": {
        "id": "DOI3lzC8hfCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in app.stream({\"query\": \"What are the risk factors of migraines\", \"chat_history\": []}):\n",
        "    print(token, end=\"\", flush=True)"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "s5Bp7j7WhfCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = app.invoke({\"query\": \"What are the symptoms of hypertension\", \"chat_history\": []})\n",
        "Markdown(response[\"generation\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "MVe1YZV8hfCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from uuid import uuid4\n",
        "\n",
        "history = {}\n",
        "session_id = str(uuid4())\n",
        "\n",
        "def chat(query):\n",
        "\n",
        "    # Initialize the chat history for the current session\n",
        "    if session_id not in history:\n",
        "        history[session_id] = []\n",
        "\n",
        "    chat_history = history[session_id]\n",
        "\n",
        "    # Invoke the app with the current query and chat history\n",
        "    result = app.invoke({\"query\": query, \"chat_history\": chat_history})\n",
        "\n",
        "    # Separate the response from the retrieved documents\n",
        "    response = result[\"generation\"]\n",
        "    documents = result[\"documents\"]\n",
        "\n",
        "    # Add the current exchange to the chat history\n",
        "    chat_history.extend([HumanMessage(content=query), AIMessage(content=response)])\n",
        "\n",
        "    if not documents:\n",
        "        return response, documents\n",
        "\n",
        "    documents = [\n",
        "        f\"{doc.page_content}\\nsource: {doc.metadata['source']}\" for doc in documents\n",
        "    ]\n",
        "\n",
        "    return response, \"\\n\\n\".join(documents)\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=chat,\n",
        "    inputs=gr.Textbox(label=\"Question\"),\n",
        "    outputs=[gr.Textbox(label=\"Response\"), gr.Textbox(label=\"Retrieved Documents\")],\n",
        "    title=\"RAG Chatbot\",\n",
        "    description=\"Ask a health-related query and the chatbot will generate a response using Retrieval Augmented Generation.\",\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True, inline=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "fNPq44pphfCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference:\n",
        "- https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_adaptive_rag_cohere.ipynb"
      ],
      "metadata": {
        "id": "YTF8xUvThfC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 style=\"text-align: center; color: #333; background-color: #f0f0f0; border-radius: 8px; border: 1px solid #ccc; padding: 10px; width: 150px;\">Thank you</h3>"
      ],
      "metadata": {
        "id": "CU7HDT3ohfC3"
      }
    }
  ]
}